{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard gpt modules + model definition\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "    \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.redidual_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        q, k, v = self.attn(x).chunk(3, dim=2)\n",
    "        q = q.view(B, T, self.n_head, self.n_embd // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.n_embd // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.n_embd // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (k.size(-1) ** (-1/2))\n",
    "        att = att.masked_fill(self.tril[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        att = att @ v\n",
    "\n",
    "        y = att.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        y = self.redidual_dropout(self.proj(y))\n",
    "        return y\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "        self.sa = SelfAttention(config)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.te = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n",
    "        return self.dropout(self.te(x) + self.pe(pos))\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = Embed(config)\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        print(\"params:\", self.get_param_count())\n",
    "    \n",
    "    def get_param_count(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x = self.embed(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 0., 1., 1., 1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (torch.randn(1, 10) * 2 > 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Looper(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.L = nn.Parameter(torch.tensor(1.5))\n",
    "\n",
    "        self.layer = layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lower = torch.floor(self.L)\n",
    "\n",
    "        for _ in range(int(lower)):\n",
    "            x = self.layer(x)\n",
    "        \n",
    "        x = x * (1 - (self.L - lower)) + self.layer(x) * (self.L - lower)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MLP1Output(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(config.n_embd, config.n_embd * 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(config.n_embd * 2, 1)\n",
    "    \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LooperCalculator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "        self.sa = SelfAttention(config)\n",
    "        self.mlp = MLP1Output(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class DynamicLooper(nn.Module):\n",
    "    def __init__(self, config,  layer, max_NL=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loop_calculator = Block(config)\n",
    "        self.max_NL = max_NL\n",
    "\n",
    "        self.layer = layer\n",
    "\n",
    "        self.avg_loop = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ls = F.sigmoid(self.loop_calculator(x)) * self.max_NL\n",
    "\n",
    "        max_L = torch.max(ls).ceil()\n",
    "\n",
    "        for i in range(int(max_L)):\n",
    "            multipliers = (ls > i).float()\n",
    "\n",
    "            x = (x * (1 - multipliers)) + self.layer(x) * multipliers\n",
    "        \n",
    "        fractional = ls - torch.floor(ls)\n",
    "\n",
    "        x = x * (1 - fractional) + self.layer(x) * fractional\n",
    "\n",
    "        self.avg_loop = 0.9 * self.avg_loop + 0.1 * ls.mean().item()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLooper(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = Embed(config)\n",
    "\n",
    "        self.blocks = nn.ModuleList([Looper(Block(config)) for _ in range(config.n_layer)])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        print(\"params:\", self.get_param_count())\n",
    "    \n",
    "    def get_param_count(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x = self.embed(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "class DynamicGPTLooper(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = Embed(config)\n",
    "\n",
    "        self.blocks = nn.ModuleList([DynamicLooper(config, Block(config)) for _ in range(config.n_layer)])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        print(\"params:\", self.get_param_count())\n",
    "    \n",
    "    def get_param_count(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x = self.embed(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 3225600\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "block_size = 128 \n",
    "n_layer = 2\n",
    "n_head = 8 \n",
    "n_embd = 256\n",
    "\n",
    "config = Config(block_size=block_size, vocab_size=vocab_size, n_layer=n_layer, n_head=n_head, n_embd=n_embd, dropout=0.1)\n",
    "model = DynamicGPTLooper(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data= train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters=10):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, base_lr=3e-4, l_lr=1e-3):\n",
    "    l_params = [param for name, param in model.named_parameters() if 'layer' in name]\n",
    "    other_params = [param for name, param in model.named_parameters() if 'layer' not in name]\n",
    "\n",
    "    param_groups = [\n",
    "        {'params': l_params, 'lr': l_lr},\n",
    "        {'params': other_params, 'lr': base_lr}\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.Adam(param_groups)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "# optimizer = get_optimizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter():\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lossi.append(loss.item())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x72a88ba0dfc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 4.4415788650512695\n",
      "\n",
      "train loss: 3.7872462272644043, val loss: 3.8145980834960938\n",
      "\n",
      "avg L values:\n",
      "4.4341209209990495\n",
      "4.027368718116302\n",
      "iter: 100, loss: 2.548718214035034\n",
      "iter: 200, loss: 2.4274096488952637\n",
      "iter: 300, loss: 2.3076932430267334\n",
      "iter: 400, loss: 2.1999266147613525\n",
      "iter: 500, loss: 2.1222760677337646\n",
      "iter: 600, loss: 2.012693166732788\n",
      "iter: 700, loss: 1.8985097408294678\n",
      "iter: 800, loss: 1.8966736793518066\n",
      "iter: 900, loss: 1.851154088973999\n",
      "iter: 1000, loss: 1.7216827869415283\n",
      "\n",
      "train loss: 1.7363102436065674, val loss: 1.8778549432754517\n",
      "\n",
      "avg L values:\n",
      "6.5191471194091815\n",
      "6.231697889165663\n",
      "iter: 1100, loss: 1.7758798599243164\n",
      "iter: 1200, loss: 1.7532345056533813\n",
      "iter: 1300, loss: 1.7457566261291504\n",
      "iter: 1400, loss: 1.6406198740005493\n",
      "iter: 1500, loss: 1.5596801042556763\n",
      "iter: 1600, loss: 1.615716814994812\n",
      "iter: 1700, loss: 1.582979440689087\n",
      "iter: 1800, loss: 1.6195865869522095\n",
      "iter: 1900, loss: 1.5613229274749756\n",
      "iter: 2000, loss: 1.5223931074142456\n",
      "\n",
      "train loss: 1.514357566833496, val loss: 1.6685121059417725\n",
      "\n",
      "avg L values:\n",
      "6.920966022205657\n",
      "6.350688719848673\n",
      "iter: 2100, loss: 1.516247272491455\n",
      "iter: 2200, loss: 1.4946746826171875\n",
      "iter: 2300, loss: 1.4985711574554443\n",
      "iter: 2400, loss: 1.47382652759552\n",
      "iter: 2500, loss: 1.4700191020965576\n",
      "iter: 2600, loss: 1.4844233989715576\n",
      "iter: 2700, loss: 1.4925358295440674\n",
      "iter: 2800, loss: 1.4531879425048828\n",
      "iter: 2900, loss: 1.460275411605835\n",
      "iter: 3000, loss: 1.4483017921447754\n",
      "\n",
      "train loss: 1.4167916774749756, val loss: 1.602638602256775\n",
      "\n",
      "avg L values:\n",
      "7.0524911341094505\n",
      "6.540510393912458\n",
      "iter: 3100, loss: 1.477097511291504\n",
      "iter: 3200, loss: 1.4306750297546387\n",
      "iter: 3300, loss: 1.4687964916229248\n",
      "iter: 3400, loss: 1.4454059600830078\n",
      "iter: 3500, loss: 1.3553972244262695\n",
      "iter: 3600, loss: 1.4165633916854858\n",
      "iter: 3700, loss: 1.4131226539611816\n",
      "iter: 3800, loss: 1.4167228937149048\n",
      "iter: 3900, loss: 1.401236891746521\n",
      "iter: 4000, loss: 1.3810738325119019\n",
      "\n",
      "train loss: 1.3485609292984009, val loss: 1.5888879299163818\n",
      "\n",
      "avg L values:\n",
      "7.2049886882605545\n",
      "6.644106124806765\n",
      "iter: 4100, loss: 1.3004543781280518\n",
      "iter: 4200, loss: 1.372010350227356\n",
      "iter: 4300, loss: 1.3671141862869263\n",
      "iter: 4400, loss: 1.3854613304138184\n",
      "iter: 4500, loss: 1.4034875631332397\n",
      "iter: 4600, loss: 1.3510890007019043\n",
      "iter: 4700, loss: 1.3203587532043457\n",
      "iter: 4800, loss: 1.3561344146728516\n",
      "iter: 4900, loss: 1.3448656797409058\n",
      "iter: 5000, loss: 1.3715038299560547\n",
      "\n",
      "train loss: 1.2858047485351562, val loss: 1.5447425842285156\n",
      "\n",
      "avg L values:\n",
      "7.2570191487457025\n",
      "6.650671471914457\n",
      "iter: 5100, loss: 1.3276954889297485\n",
      "iter: 5200, loss: 1.2806005477905273\n",
      "iter: 5300, loss: 1.3499842882156372\n",
      "iter: 5400, loss: 1.3466705083847046\n",
      "iter: 5500, loss: 1.354819655418396\n",
      "iter: 5600, loss: 1.3751519918441772\n",
      "iter: 5700, loss: 1.3363875150680542\n",
      "iter: 5800, loss: 1.321459174156189\n",
      "iter: 5900, loss: 1.293975591659546\n",
      "iter: 6000, loss: 1.35934317111969\n",
      "\n",
      "train loss: 1.2514641284942627, val loss: 1.542799711227417\n",
      "\n",
      "avg L values:\n",
      "7.329369244368724\n",
      "6.6337671027373615\n",
      "iter: 6100, loss: 1.2951704263687134\n",
      "iter: 6200, loss: 1.2599152326583862\n",
      "iter: 6300, loss: 1.303692102432251\n",
      "iter: 6400, loss: 1.278747320175171\n",
      "iter: 6500, loss: 1.3042871952056885\n",
      "iter: 6600, loss: 1.2973451614379883\n",
      "iter: 6700, loss: 1.349010944366455\n",
      "iter: 6800, loss: 1.2621809244155884\n",
      "iter: 6900, loss: 1.3123011589050293\n",
      "iter: 7000, loss: 1.280623197555542\n",
      "\n",
      "train loss: 1.2289237976074219, val loss: 1.542149305343628\n",
      "\n",
      "avg L values:\n",
      "7.409663696406178\n",
      "6.705187448447855\n",
      "iter: 7100, loss: 1.310913324356079\n",
      "iter: 7200, loss: 1.2372405529022217\n",
      "iter: 7300, loss: 1.2216250896453857\n",
      "iter: 7400, loss: 1.2762460708618164\n",
      "iter: 7500, loss: 1.295946717262268\n",
      "iter: 7600, loss: 1.240801215171814\n",
      "iter: 7700, loss: 1.2714474201202393\n",
      "iter: 7800, loss: 1.2612407207489014\n",
      "iter: 7900, loss: 1.256876826286316\n",
      "iter: 8000, loss: 1.2586896419525146\n",
      "\n",
      "train loss: 1.220510482788086, val loss: 1.5302047729492188\n",
      "\n",
      "avg L values:\n",
      "7.421498364544861\n",
      "6.681189609083952\n",
      "iter: 8100, loss: 1.2303041219711304\n",
      "iter: 8200, loss: 1.200087308883667\n",
      "iter: 8300, loss: 1.2001041173934937\n",
      "iter: 8400, loss: 1.2317568063735962\n",
      "iter: 8500, loss: 1.2077844142913818\n",
      "iter: 8600, loss: 1.249001383781433\n",
      "iter: 8700, loss: 1.2582148313522339\n",
      "iter: 8800, loss: 1.2205291986465454\n",
      "iter: 8900, loss: 1.2219749689102173\n",
      "iter: 9000, loss: 1.2243901491165161\n",
      "\n",
      "train loss: 1.1636625528335571, val loss: 1.5053249597549438\n",
      "\n",
      "avg L values:\n",
      "7.4883565685879585\n",
      "6.6762600789502775\n",
      "iter: 9100, loss: 1.2992961406707764\n",
      "iter: 9200, loss: 1.1727224588394165\n",
      "iter: 9300, loss: 1.181694507598877\n",
      "iter: 9400, loss: 1.258802890777588\n",
      "iter: 9500, loss: 1.1935237646102905\n",
      "iter: 9600, loss: 1.1764837503433228\n",
      "iter: 9700, loss: 1.2648409605026245\n",
      "iter: 9800, loss: 1.1783515214920044\n",
      "iter: 9900, loss: 1.2420889139175415\n",
      "iter: 10000, loss: 1.2292015552520752\n",
      "\n",
      "train loss: 1.1544859409332275, val loss: 1.4992311000823975\n",
      "\n",
      "avg L values:\n",
      "7.524084177472295\n",
      "6.672793001278366\n"
     ]
    }
   ],
   "source": [
    "for i in range(10001):\n",
    "    loss = iter()\n",
    "    if i % 100 == 0:\n",
    "        print(f\"iter: {i}, loss: {loss.item()}\")\n",
    "    if i % 1000 == 0:\n",
    "        print()\n",
    "        losses = estimate_loss()\n",
    "        print(f\"train loss: {losses['train']}, val loss: {losses['val']}\")\n",
    "        print()\n",
    "\n",
    "        print(\"avg L values:\")\n",
    "        for layer in model.blocks:\n",
    "            print(layer.avg_loop)\n",
    "\n",
    "        # print(\"L values:\")\n",
    "        # for layer in model.blocks:\n",
    "        #     print(layer.L.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]], device='cuda:0')\n",
      "\n",
      "\n",
      "MENENIUS:\n",
      "For whence I was fair for your oils.\n",
      "\n",
      "BISHOP OF ELY:\n",
      "Am I; ay, your eyes look'd off my husbandry;\n",
      "In Petruchio, that it is most done,\n",
      "He stands upon this hour disdains--took\n",
      "Long in her husband; and if power you pray a king,\n",
      "And way my right.\n",
      "\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "def generate(max_new=256):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(encode(\"\\n\"), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print(idx)\n",
    "    for _ in range(max_new):\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits, loss = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    # decode and print\n",
    "    out = idx.squeeze().tolist()\n",
    "    out = decode(out)\n",
    "\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "print(generate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
